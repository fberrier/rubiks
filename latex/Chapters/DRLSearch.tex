% Chapter Template

\chapter{Deep Reinforcement Learning Search} % Main chapter title

\label{Chapter1} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

In this section, I will succintly go through the different techniques that underly the algorithms I have implemented and compared in this project. Along the way, I will give some references for the interested reader.


%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\section{Graph Search \& Heuristics}

\label{GSH}


It is quite fruitful to think of single-player, deterministic, fully-observable puzzles such as the \textbf{SP} and \textbf{RC} as unit-cost graphs. We start from an initial \textit{node} (usually some scrambled configuration of the \textbf{SP} or \textbf{RC}), and via legal moves, we transition to new nodes, until hopefully we manage to get to the goal in the shortest number of moves possible. 
\\
\\
Graphs search (or graphs traversal) is a rather large, sophisticated and mature branch of \textbf{CS} that studies strategies to find \textit{optimal} paths from initial to goal node in a graph. What is meant by \textit{optimal} can vary, but usually is concerned with one or several of minimizing the run-time of the strategies, their memory complexity/usage or the total cost/length of the solutions they come up with. In the general theory, different transitions can have different costs associated with them. In this project however, we can reasonably consider that each move of a tile in the \textbf{SP} or each rotation of a face in the \textbf{RC} might are taking a constant and identical amount of time and effort to perform, and I will therefore consider all the graphs to be unit-cost (i.e. all moves have cost 1).
\\
\\
Search strategies typically maintain a frontier, which is the collection of unexpanded nodes so far. They keep expanding the frontier, by choosing the next node to expand (expanding a node simply means adding all of its children nodes - i.e. those which are reachable via legal transitions - to the frontier for future evaluation) until a solution is found. The question is how to choose the next node to expand! Some graphs search strategies are said to be \textit{uninformed}, in the sense that they do not exploit any domain-specific knowledge or insight about what the graphs represent to choose the next node to expand. In that category fall for instance Breadth First Search (\textbf{BFS}) and Depth First Search (\textbf{DFS}), which as their name suggest expand nodes from the frontier based on, respectively, a \textbf{FIFO} and \textbf{LIFO} policy. \textbf{BFS} and \textbf{DFS} will only work for modest problems where the number of transitions and states are reasonably small; they can however work in a wide variety of situations as they make no assumptions and require no knowledge. It is quite easy to see that \textbf{BFS} is an \textit{optimal} search strategy, in that when it does find a solution, that solution is of smallest possible cost (i.e. optimal). \textbf{DFS} is obvisouly not optimal as it could very well find a solution very deep in the graphs while expanding the very first branch, not realising that a solution was 1 level away from the start on another branch!
\\
\\
\textit{Informed} strategies, on the other hand, exploit domain-specific knowledge. One very popular such strategy is \textbf{A$^{*}$} (see \cite{DBLP:journals/jacm/DechterP85}), which always first examine the node of smallest expected total cost (\textbf{f}), itself the sum of the cost-so-far from initial to current node (\textbf{g}) plus the expected cost-to-go from current node to solution (\textbf{h}). The expected cost-to-go \textbf{h} is often called a \textit{heuristic}. The better the heuristic is, the better \textbf{A$^{*}$} usually performs. An important property of heuristics is \textit{admissibility}. In short, admissible heuristics never over-estimate the real cost-to-go, and can easily be shown to render \textbf{A$^{*}$} optimal.


%-----------------------------------
%	SECTION 2
%-----------------------------------


\section{Reinforcement Learning}

As mentioned in the abstract, \textbf{RL} (see \cite{Sutton1998} for a brilliant exposition of the basic concepts and theory) has known a bit of false start in the 1950s and 1960s but recently been extremely successfully applied to a variety of problems (from Atari to board games and puzzles, robotics and else). \textbf{RL} is concerned with how intelligent agents can learn optimal decisions from observing/experiencing rewards while interacting in their environment. One of the early concept in the field is that of value function $s \to \textbf{V}(s)$, which tells us the maximum expected reward the agent can obtain from a given state $s$ (if it takes the optimal decisions). In discrete states and transitions spaces, the value function can be remarkably computed (or at least approximated) by a rather mechanical and magic-like procedure called value-iteration (which uses the equivalent of the Bellman equation from optimal control), where each state $s$'s value is iteratively refined by combining the value of $s$'s reachable states.



%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------



\section{Deep Learning}

Quite similarly to \textbf{RL}, Deep Learning (\textbf{DL}) has not initially had the success the \textbf{AI} community was hoping it would. Marvin Minsky, who often took the blame for having \textit{killed} funding into the field, even regretted writing his 1969 book (\cite{minskypapert69}) in which he had proved that three-layer feed forward perceptrons were not quite the universal functions approximators some had hypothesized them to be. Since around 2006 (see \cite{GoodBengCour16} for more history of the recent burgeoning of \textbf{DL}), the field has known a rebirth, probably due to combination of factors, from ever more powerful computers and larger storage, the availability of data sets large and small on the internet, the advances in many techniques and heuristics (backward propagation, normalisation, drop-outs, different optimisation schemes, etc...) and the huge amount of experimentation with different architectures (from very deep feed forward fully connected networks, to convolutional, recurrent and more exoctic networks). It is not an exageration to say that \textbf{DL} has rendered obsolete entire fields of research and bodies of knowledge, most notably in \textbf{CV}, robotics, computational biology and \textbf{NLP}. \textbf{DL} has often become the method of choice to learn or approximate highly dimensional functions or random processes. The beauty of it is that the relevant features are autodiscovered during the training of the network, via back-ward propagation and trial-and-error, rather than having to be postulated or handcrafted as is often the case with other \textbf{ML} techniques.

%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------

\section{DL and DLR Heuristics}

So what is the link between all of these - A$^{*}$'s heuristics, \textbf{DL}, \textbf{RL}'s value iteration?
\\
\\
Sometimes we have ways of computing good solutions/heuristics to our puzzles, but cannot computationally do this for all possible states (maybe the state space is simply too large!). One approach in that case is to compute the solution/heuristic for some states, and let a \textbf{DL} model learn how to extrapolate to the rest of the state space. This is a case of \textit{supervised} learning in some sense, since we need a teacher-solver to tell us good solutions or heuristics to extrapolate from.
\\
\\
An alternative approach in order to compute the cost-to-go is via \textbf{RL} value-iteration. Indeed, minimizing the cost-to-go (heuristic) is equivalent to maximising the cumulative reward given by -1 for each transition not reaching the goal state and +1 for the last transition to the goal. Clearly these two quantities are equivalent (modulo a trivial linear transformation). We can perform this value-iteration in an unsupervised way: we do not need a teacher to tell us the target value of the value function/cost-to-go but instead learn it simply from refining a state $s$'s value by replacing it at each iteration by the minimum of its current value, and the minimum value of all reachable states from $s$ polus one (since it takes one move to reach them). That is, for $t \in \mathbb{N}^{*}, s \in \mathcal{S}$, the state space, we update $V_{t}(s)$ via:
\begin{equation} \label{eq:DLVI}
\begin{split}
& V_{t+1}(s) \leftarrow min \Big( V_{t}(s), 1 + min_{s' \in \mathcal{C}(s)}(V_{t}(s'))  \Big) \\
& V_{t+1}(s) = 0 if s \in \mathcal{G}
\end{split}
\end{equation}
where $\mathcal{C}(s)$ are the children (reachable states via 1 move) of $s$ and $\mathcal{G}$ are the goal states.
\\
\\
The state space is often so large however, that we cannot practically perform this procedure for all states. This is where \textbf{DL} comes again to the rescue to act as function approximator, combining with \textbf{RL} into what has become known as \textbf{DRL}. The subtelty is then that both the left-hand-side and right hand side in the value-iteration procedure above are given by a \textbf{DL} network which we are constantly tweaking by back-ward propagation. For my implementation of \textbf{DRL}, I followed the same approach as in \cite{Mnih2013}, where the right hand-side of \ref{eq:DLVI} is a fixed \textit{target-network}. The left hand side is another \textit{current-network} which alone is modified by the \textbf{DL} learning (backward propagation and optimization), until convergence is deemed reached. We then update the \textit{target-network} by a copy of the \textit{current-network}, and then keep iterating again, until the whole process is deemed to have converged.
\\
\\
For this project, I have implemented this procedure in a class called DeepReinforcementLearner (see section \ref{sec:codelearners} for details)


%----------------------------------------------------------------------------------------
%	SECTION 5
%----------------------------------------------------------------------------------------

\section{Deep Q-Learning}

Another concept of \textbf{RL} is that of Q-function $Q(s, a)$, the maximum expected value the agent can obtain from state $s$, taking action $a$ and acting optimally afterwards. Notice that the knowledge of $V$ everywhere is equivalent to that of $Q$, since obviously:
\begin{equation} \label{eq:QI}
V(s) = max_{a \in \mathcal{A}(s)} Q(s, a)
\end{equation}
where $\mathcal{A}(s)$ are the transitions/actions we can take from state $s$. Conversely, we can recover $Q$ from $V$ since:
\begin{equation} \label{eq:QI2}
Q(s, a) = R(a) + V(a(s))
\end{equation}
where $R(a)$ is the reward of performing action $a$ and $a(s)$ is the state we reach by performing action $a$ from state $s$. Similarly to the way one can compute $V$ via value-iteration (at least in finite settings), $Q$ can also be computed by iteration (See \cite{Watkins1992} for details of convergence of that procedure).
\\
\\
Similarly to our discussion about \textbf{DRL}, in cases where the state space is large, it can be useful to approximate $Q$, or some related quantity (such as e.g. a probability distribution over the actions that can be taken from $s$) by a neural network, and use a similar procedure to that of the previous section to update the right-hand-side and left-hand-side of the iterations. That becomes \textbf{DQL}!
\\
\\
For this project, I have implemented the procedure described in \cite{https://doi.org/10.48550/arxiv.1805.07470} in a class called DeepQLearner (see section \ref{sec:codelearners} for details). The network jointly learns the value function (heuristic, or cost-to-go) as well as a probability distribution over the actions.

%----------------------------------------------------------------------------------------
%	SECTION 6
%----------------------------------------------------------------------------------------

\section{Monte Carlo Tree Search}

In my implementation of \textbf{DQL}, which follows \cite{https://doi.org/10.48550/arxiv.1805.07470} (and is also the same procedure followed by alpha-go \cite{AlphaGo}), the output of the neural network that is learnt by \textbf{DQL} is no longer just a cost-to-go estimate as with \textbf{DRL}, but a joint cost-to-go and distribution over actions (to be concrete, that means a 5-dimensional vector for the \textbf{SP} and a 13-dimensional vector for the \textbf{RC}). This leaves us with the question of how can we use the output of such a network to search for a solution. One simple answer (which I have tried) is that we could disregard the distribution and only take into account the cost-to-go by simply using $A^{*}$. The rationale for doing so is that we might hope that jointly learning the cost-to-go and the actions via a combination of MSE and cross-entropy-loss leads to better heuristics, since we use more information during the learning and iterations.
\\
\\
Another possibility is to use an algorithm which combines both the actions distribution and the value function to inform its search. The Monte Carlo Tree Search (\textbf{MCTS}) is such an algorithm. In short, it generates at each step (and possibly in a distributed manner) paths to a \textit{leaf} on the frontier of unexpanded nodes, expands that node and checks if it is a goal state (and keeps going until the goal is found). The paths followed from initial state to leaves are decided via a combination of the probability distribution over actions (following the most promising actions) as well as the value function. In order to prevent always following the same paths (which might take us very far from the goal), \textbf{MCTS} algorithms usually keep track of actions they have followed already and underweight their probability so that subsequent paths explore other actions.
\\
I have implemented the exact \textbf{MCTS} procedure described in \cite{https://doi.org/10.48550/arxiv.1805.07470} in a class called MonteCarloTreeSearch (see section \ref{sec:codesolvers} for details).




































