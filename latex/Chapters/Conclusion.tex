% Chapter Template

\chapter{Conclusion \& Professional Issues} % Main chapter title

\label{Conclusion} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------


\Section{Learning}
Working on this project has been overall a lot of fun. Even setting aside implementation and computer-experimentation for a moment, I have learnt a great deal about puzzles in general and how to approach thinking about them in a more structured fashion. Specifically about the Rubik's cube, I have even spent the time to learn a few algorithms that are sufficient to solve them by hand. I am now able to solve the 2x2x2 under one minute, 3x3x3 usually around 2 minutes and the 4x4x4 in about 5 minutes. More specifically regarding the techniques discussed in this project, the principles that underly uninformed and informed search, A$^{*}$ and heuristics, multi-stage solvers, and of course the various deep learning methods I have played with (\textbf{DL}, \textbf{DRL} and \textbf{DQL}) all extend to many other puzzles and dimensions and are a great new addition to my tool-set and knowledge.

\Section{Implementation}
I am quite satistified to have had the time to implement and play with so many solvers and methods, spanning so many different \textbf{CS} and \textbf{AI} paradigms, from handcrafting my own \textbf{SP} \textit{naive} solver, playing with Kociemba, to implementing \textbf{BFS}, \textbf{DFS}, A$^{*}$, admissible heuristics such as Manhattan for the \textbf{SP} and improving on it via linear constraints penalty, and of course all the machinery necessary to fit, train and exploit some \textbf{DL}, \textbf{DRL} and \textbf{DQL} heuristics to guide search algorithms such as  A$^{*}$ and \textbf{MCTS}. Ideally I would have liked to implement things in C++ for speed and multithreading (especially for \textbf{MCTS}) but time was limited so I decided to prioritize breadth and ease of experimentation over depth and efficiency of implementation. The choice of Python definitely made it much easier to integrate with many of the ecosystem's libraries (matplotlib, pandas and pytorch in particular).

\Section{Deep Reinforcement-Q Learning}
It will not come as a surprise to anyone who has played with them that Deep Reinforcement Learning and Deep Q Learning models can be quite capricious to train: choosing the convergence mechanisms and criteria, the interplay of generation of new training data with the inner and outer loop of the \textbf{DRL} and \textbf{DQL} algorithms (see \ref{sec:TheoryDLDRL} and \ref{sec:TheoryDQL}) seemed more of an art than a sicence at times. Obviously a lot more tuning, as well as much longer training time and computing resources would have been necessary to make these techniques more competitive at higher dimensions, but overall I remain extremely impressed by how, all things considered - and given how daunting the project seemed at first - it took me little effort to train successful \textbf{DRL} and \textbf{DQL} models capable of solving puzzles with such large state and action spaces.

\Section{Professional issues}
My favourite principle within the BCS code of conduct (\cite{BCS}) is to `show what you know, learn what you don't'. The whole point of this MSc in AI, after a 20 year career, most of which spent in quantitative trading, was indeed for me to revise old skills, learn some new ones and further my knowledge in \textbf{ML} and \textbf{AI}. That has the `learn what you don't' covered. As for the `show what you know', I have endeavoured to show honest, and therefore in some cases modest, results. Ideally I would have liked to be able to try more methods and go further with the \textbf{DxL} methods on the 3x3x3 \textbf{RC} as well as higher dimensional \textbf{SP} like the 5x5, but this was sadly not possible within the time imparted.
\\
\\
As for issues related to ethics and safety, some of which are outined in the ACM code of ethics (\cite{ACM}), anyone working in \textbf{AI} should in my opinion be acutely aware of them. There is clearly some divide in the community regarding the dangers of \textbf{AI}, but I myself stand firmly in the Bostrom camp (see in particular \cite{https://doi.org/10.1111/1758-5899.12718} and \cite{Bostrom2014}). His paper-clip-maximiser is one of the most thought provoking ideas and does show that the most benign looking \textbf{AI} agent, if sufficiently capable, could create disproportionate harm. To conclude on a humorous (?) note, we probably would not want either to see a Rubik's solver engulf all of the earth's and galaxy's resources in order to compute the nxnxn Rubik's God number for ever growing values of n.
