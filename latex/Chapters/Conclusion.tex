% Chapter Template

\chapter{Conclusion \& Professional Issues} % Main chapter title

\label{Conclusion} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------


\Section{Learning}
Working on this project has been overall a lot of fun. Even setting aside implementation and computer-experimentation for a moment, I have learnt a great deal about puzzles in general and how to approach thinking about them in a more structured fashion. Specifically about the Rubik's cube, I have even spent the time to learn a few algorithms that are sufficient to solve them by hand. I am now able to solve the 2x2x2 under one minute, 3x3x3 usually around 2 minutes and the 4x4x4 in about 5 minutes. More specifically regarding the techniques discussed in this project, the principles that underly uninformed and informed search, A$^{*}$ and heuristics, multi-stage solvers, and of course the various deep learning methods I have played with (\textbf{DL}, \textbf{DRL} and \textbf{DQL}) all extend to many other puzzle and dimensions and are a great new tool-set to add to my knowledge and experience.

\Section{Implementation}
I am quite satistified to have had the time to implement and play with so many solvers and methods, spanning so many different \textbf{CS} and \textbf{AI} paradigms, from handcrafting my own \textbf{SP} \textit{naive} solver, playing with Kociemba, to implementing \textbf{BFS}, \textbf{DFS}, A$^{*}$, admissible heuristics such as Manhattan for the \textbf{SP} and improving on it via linear constraints penalty, and of course all the machinery necessary to fit, train and exploit some \textbf{DL}, \textbf{DRL} and \textbf{DQL} heuristics to guide search algorithms such as  A$^{*}$ and \textbf{MCTS}. Ideally I would have liked to implement things in C++ for speed and multithreading (especially for \textbf{MCTS}) but time was limited so I decided to prioritize breadth and ease of experimentation over depth and efficiency of implementation. The choice of Python definitely made it much easier to integrate with many of the ecosystem's libraries (matplotlib, pandas and pytorch in particular).

\Section{Deep Reinforcement-Q Learning}
It will not come as a surprise to anyone who has played with them that Deep Reinforcement Learning and Deep Q Learning models can be quite capricious to train: choosing the convergence mechanisms and criteria, the interplay of generation of new training data with the inner and outer loop of the \textbf{DRL} and \textbf{DQL} algorithms (see \ref{sec:TheoryDLDRL} and \ref{sec:TheoryDQL}) seemed more of an art than a sicence at times. Obviously a lot more tuning, as well as much longer training time and computing resources would have been necessary to make these techniques more competitive at higher dimensions, but overall I remain extremely impressed by how, all things considered - and given how daunting the project seemed at first - it took me little effort to train successful \textbf{DRL} and \textbf{DQL} models capable of solving puzzles with such large state and action spaces.

\Section{Professional issues}
Bug in Kociemba and software design\\
AI safety 
See \cite{ManyWorlds}
See \cite{https://doi.org/10.1111/1758-5899.12718} and \cite{Bostrom2014}
