% Chapter Template

\chapter{Conclusion \& Professional Issues} % Main chapter title

\label{Conclusion} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------


\textbf{Learning} -- Working on this project has been a lot of fun. Setting aside implementation and experimentation for a moment, I have learnt a great deal about puzzles in general and how to think about them in a structured fashion. I have also spent the time to learn a few \textbf{RC} algorithms allowing me to solve the 2x2x2 in under one minute the 3x3x3 in 2 minutes and the 4x4x4 in about 5 minutes. More specifically regarding the techniques discussed in this project, the principles that underly blind and informed search, A$^{*}$ and heuristics, multi-stage solvers, and of course the various \textbf{AI} methods I have played with (\textbf{DL}, \textbf{DRL} and \textbf{DQL}) all are a great new addition to my tool-set and knowledge.
\\
\\
\textbf{Implementation} -- I am quite satistified to have implemented and played with so many solvers and methods, spanning different \textbf{CS} and \textbf{AI} paradigms, from handcrafting my own \textbf{SP} \textit{naive} solver, playing with Kociemba, to implementing \textbf{BFS}, \textbf{DFS}, A$^{*}$, admissible heuristics such as Manhattan with linear constraints penalty, and of course all the machinery necessary to fit, train and exploit some \textbf{DL}, \textbf{DRL} and \textbf{DQL} heuristics and even \textbf{MCTS}. Ideally I would have liked to implement things in C++ for speed and multithreading but time was limited so I decided to prioritize breadth and ease of experimentation over depth and efficiency of implementation. Python made it easier to integrate many of the ecosystem's libraries e.g. matplotlib, pandas and pytorch.
\\
\\
\textbf{Deep Reinforcement-Q Learning} -- Anyone who has played with \textbf{DL}, \textbf{DRL} and \textbf{DQL} models (\ref{sec:TheoryDLDRL} and \ref{sec:TheoryDQL}) know they can be capricious to train: choosing the convergence criteria, the interplay of training data generation with the inner and outer loops felt more art than sicence. More tuning, longer training time and computing resources would have been necessary to make these techniques more competitive at higher dimensions, but I remain extremely impressed by how, all things considered - and given how daunting the project seemed at first - it took me little effort to train successful \textbf{DRL} and \textbf{DQL} models capable of solving large state and action spaces puzzles.
\\
\\
\textbf{Professional issues} -- The BCS code of conduct (\cite{BCS}) tells us to `show what you know, learn what you don't'. Furthering my \textbf{ML} and \textbf{AI} knowledge with this MSc in AI probably has the `learn what you don't' part covered. As for the `show what you know', I have endeavoured to show honest and sometimes modest, results. I would have liked to go further with the \textbf{DxL} methods on the 3x3x3 \textbf{RC} but this was sadly not possible within the time imparted. As for issues of ethics and safety, outined in the ACM code of ethics (\cite{ACM}), anyone working in \textbf{AI} should in my opinion be aware of them. There is some divide in the community regarding the dangers of \textbf{AI}. I myself firmly stand in the Bostrom camp (see \cite{https://doi.org/10.1111/1758-5899.12718} and \cite{Bostrom2014}). His thought-provoking paper-clip-maximiser shows that the most benign looking \textbf{AI} agent, if sufficiently capable, could create disproportionate harm. We probably do not want either to see a Rubik's solver engulf all of the earth's and galaxy's resources to compute the nxnxn Rubik's God number for ever growing values of n :)
