% Chapter Template

\chapter{Deep Reinforcement Learning Search} % Main chapter title

\label{Chapter1} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

In this chapter, I succinctly go through the different techniques underlying the algorithms I have implemented for this project and present them in simplified pseudo-code. Along the way, I will give some references for the interested reader.


%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------

\Section{Graphs Search \& Heuristics}

\label{GSH}
\label{sec:TheoryBFSDFS}
\label{sec:TheoryAStar}

It is fruitful to think of single-player, deterministic, fully-observable puzzles such as the \textbf{SP} and \textbf{RC} as unit-cost graphs. We start from an initial \textit{node} (usually some scrambled configuration of the \textbf{SP} or \textbf{RC}), and via legal moves, we transition to new nodes, until hopefully we manage to get to the goal in the shortest number of moves possible. 
\\
\\
Graphs search (or graphs traversal) is a rather large, sophisticated and mature branch of \textbf{CS} that studies strategies to find \textit{optimal} paths from initial to goal node in a graph. What is meant by \textit{optimal} can vary, but usually is concerned with one or several of minimizing the run-time of the strategies, their memory complexity/usage or the total cost/length of the solutions they come up with. In the general theory, different transitions can have different costs associated with them. In this project however, we can reasonably consider that each move of a tile in the \textbf{SP} or each rotation of a face in the \textbf{RC} are taking a constant and identical amount of time and effort to perform, and I will therefore consider all the graphs to be unit-cost (i.e. all moves have cost 1).
\\
\\
Search strategies typically maintain a frontier, which is the collection of unexpanded nodes. They keep expanding the frontier, by choosing the next node to process (by adding all of its children nodes - i.e. those which are reachable via legal transitions - to the frontier for future evaluation) until a solution is found. The question is how to choose the next node to expand! Some graphs search strategies are said to be \textit{blind}, in the sense that they do not exploit any domain-specific knowledge or insight about what the graphs represent to choose the next node to expand. In that category fall Breadth First Search (\textbf{BFS}) and Depth First Search (\textbf{DFS}), which as their name suggest expand nodes from the frontier based on, respectively, a \textbf{FIFO} and \textbf{LIFO} policy (see pseudo-code \ref{alg:TheoryBFSDFS}). \textbf{BFS} and \textbf{DFS} will only work for modest problems where the number of transitions and states are reasonably small; they can however work in a wide variety of situations as they make no assumptions and require no knowledge. It is quite easy to see that \textbf{BFS} is an \textit{optimal} search strategy, in that when it does find a solution, that solution is of smallest possible cost (i.e. optimal). \textbf{DFS} is obvisouly not optimal as it could very well find a solution very deep in the graph while expanding the very first branch, not realising that a solution was one level away from the root on the next branch!
\\
\\
\noindent \textit{Informed} strategies (see pseudo-code \ref{alg:TheoryAStar}) on the other hand exploit domain-specific knowledge. One popular algorithm is \textbf{A$^{*}$} (see \cite{DBLP:journals/jacm/DechterP85}), which first examines the node of smallest expected total cost (\textbf{f}), itself the sum of the cost-so-far from initial to current node (\textbf{g}) plus the expected cost-to-go from current node to goal (\textbf{h}). The expected cost-to-go \textbf{h} is called a \textit{heuristic}. The better the heuristic, the better \textbf{A$^{*}$} performs. An important property of heuristics is \textit{admissibility}: admissible heuristics never over-estimate the real cost-to-go, and can easily be shown to render \textbf{A$^{*}$} optimal.
\teal
\begin{algorithm}[H]
\caption{Blind Graphs Search -- BFS \& DFS}\label{alg:TheoryBFSDFS}
\begin{algorithmic}
\Function{BlindSearch}{$root, time\_out, search\_type=Search.BFS$}
\State{\Call{InitialiseTimeOut}{$time\_out$}}
\If{\Call{IsGoal}{$root$}}
  \Return{$root$}
\EndIf
\State $explored \gets \{\}$
\State $frontier \gets [root]$
\While{true}
\State\Call{CheckTimeOut}{void}
 \If{\Call{Empty}{frontier}}
  \Return{$failure$}
 \EndIf
 \If{\Call{Equal}{$search\_type, Search.BFS$}}
 \State $node \gets \Call{PopFront}{$frontier$}$
 \Else
 \State $node \gets \Call{PopBack}{$frontier$}$ \teal \algorithmiccomment{$Search.DFS$} \black
 \EndIf
\State \Call{Add}{$explored, node$}
\For{$child \in $ \Call{Children}{$node$}}
\If{\Call{IsGoal}{$child$}}
\Return{$child$}
\EndIf
\State \Call{Add}{$frontier, child$}
\EndFor
\EndWhile
\EndFunction
\end{algorithmic}
\end{algorithm}
\black
\teal
\begin{algorithm}[H]
\caption{Informed Graphs Search -- A$^{*}$}\label{alg:TheoryAStar}
\begin{algorithmic}
\Function{A$^{*}$}{$root, time\_out, H$} \teal \algorithmiccomment{H -- cost-to-go heuristic} \black
\State\Call{InitialiseTimeOut}{$time\_out$}
\State $f \gets 0 + \Call{H}{$node$}$
\State $explored \gets \{\}$
\State $frontier \gets \Call{SortedMultiMap}{\{cost: node\}}$
\While{true}
\State\Call{CheckTimeOut}{void}
 \If{\Call{Empty}{frontier}}
  \Return{$failure$}
 \EndIf
\State $(cost, node) = \Call{PopSmallest}{frontier}$
\If{\Call{IsGoal}{$node$}}
\Return{$node$}
\EndIf
\State \Call{Add}{$explored, node$}
\For{$transition, child \in $ \Call{Children}{$node$}}
	\State $child\_cost \gets \Call{Cost}{transition} + \Call{H}{child}$
            \If{$child \in frontier \ and \ child\_cost \le cost$} \\
             \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \ \Call{UpdateCost}{$frontier, child, child\_cost$}
            \ElsIf{$child \notin explored$}
                \Call{Add}{$frontier, child, child\_cost$}
	\EndIf
\EndFor
\EndWhile
\EndFunction
\end{algorithmic}
\end{algorithm}
\black

%-----------------------------------
%	SECTION 2
%-----------------------------------


\Section{Reinforcement Learning}
\label{sec:RLTheory}

\textbf{RL} (see \cite{Sutton1998} for a brilliant exposition of the basic concepts and theory) has known a bit of false start in the 1950s and 1960s but recently been extremely successfully applied to a variety of problems (from Atari to board games and puzzles, robotics, \dots). \textbf{RL} is concerned with how intelligent agents can learn optimal decisions from experiencing rewards while interacting in their environment. One of the fundamental concept in the field is that of value function $s \to \textbf{V}(s)$, which tells us the maximum expected reward - or equivalently and better suited to our puzzle solving task, the minimum cost - the agent can obtain from a given state $s$ (assuming optimal decisions). In finite state and transitions spaces, the value function can be remarkably computed by a mechanical, rather magic-like procedure called value-iteration (equivalent to Optimal Control's Bellman equation, see pseudo-code below \ref{alg:RLTheory}), where each state $s$'s value is iteratively refined by combining the value of $s$'s reachable children states.


\teal
\begin{algorithm}[H]
\caption{Reinforcement Learning -- Value Iteration}\label{alg:RLTheory}
\begin{algorithmic}
\Function{ValueIteration}{$states$} \teal \algorithmiccomment{states -- generator of states} \black
\State $V \gets \{state: \infty \ for \ state \in states\}$
\State $change \gets true$
\While{$change$}
\State $change \gets False$
\For{$state, cost \in states$}
	\State $V[state] \gets \infty$
	\For{$child, t\_cost \in \Call{Children}{state}$}
	\State $V[state] \gets \Call{Min}{V[state], V[child] + t\_cost}$
	\EndFor
           \If{$V[state] \neq cost$}
		\State{$change \gets true$}
	\EndIf
\EndFor
\EndWhile
\Return{$V$}
\EndFunction
\end{algorithmic}
\end{algorithm}
\black


%----------------------------------------------------------------------------------------
%	SECTION 3
%----------------------------------------------------------------------------------------



\Section{Deep Learning}

Similarly to \textbf{RL}, \textbf{DL} has not initially had the success the \textbf{AI} community was hoping it would. Marvin Minsky, who often took the blame for having \textit{killed} funding into the field, even regretted writing his 1969 book (\cite{minskypapert69}) in which he had proved that three-layer feed forward perceptrons were not quite the universal functions approximators some had hypothesized them to be. Since around 2006 (see \cite{GoodBengCour16} for more history of the recent burgeoning of \textbf{DL}), the field has known a rebirth, probably due to a combination of factors, from ever more powerful computers and larger storage, the availability of data sets large and small on the internet, the advances in many techniques and heuristics (backward propagation, normalisation, drop-outs, different optimisation schemes, etc \dots) and the huge amount of experimentation with different architectures (from very deep feed forward fully connected networks, to convolutional, recurrent and more exoctic networks). It is not an exageration to say that \textbf{DL} has revolutionized entire fields of research such as computer vision, robotics, computational biology and \textbf{NLP}. \textbf{DL} has often become the method of choice to learn or approximate highly dimensional functions or random processes. The beauty of it is that the relevant features are autodiscovered during the training of the network, via back-ward propagation and trial-and-error, rather than having to be postulated or handcrafted as is often the case with other \textbf{ML} techniques.

%----------------------------------------------------------------------------------------
%	SECTION 4
%----------------------------------------------------------------------------------------

\Section{DL and DLR Heuristics}
\label{sec:TheoryDLDRL}
So what is the link between all of these: A$^{*}$'s heuristics, \textbf{DL}, \textbf{RL}'s value iteration? Sometimes we have ways of computing good solutions/heuristics to our puzzles, but cannot computationally do this for all states (the state space might be too large!). One approach in that case is to compute solutions for \textit{some manageable number of} states, and let a \textbf{DL} model learn how to extrapolate to the whole state space. This is a case of \textit{supervised} learning, since we need a teacher-solver to tell us good solutions or heuristics to extrapolate from. Pseudo-code is provided below, see \ref{alg:TheoryDLDRL}:

\newlength\mylen
\settowidth\mylen{DeepLearningHeuristic(}
\teal
\begin{algorithm}[H]
\caption{Deep Learning -- Heuristic}\label{alg:TheoryDLDRL}
\begin{algorithmic}
\Function{DeepLearningHeuristic}{**args}
\State{$puzzle\_type \gets \Call{FromParams}{**args}$}
\State{$n\_puzzles \gets$ \Call{FromParams}{**args}}
\State{$H \gets \Call{FromParams}{**args}$} \teal{\algorithmiccomment{H -- teacher heuristic to extrapolate via \textbf{DL}}}\black
\State{$loss\_function \gets \Call{FromParams}{**args}$}
\State{$net\_architecture \gets \Call{FromParams}{**args}$}
\State $V = \{state: \Call{H}{state} \ for \ state \in \Call{GenerateRandomStates}{puzzle\_type, n\_puzzles}\}$
\ \ \ \ \Return{$\Call{TrainNeuralNetwork}{V, loss\_function, net\_architecture}$}
\EndFunction
\end{algorithmic}
\end{algorithm}
\black
\noindent An alternative approach to compute the cost-to-go is via \textbf{RL} value-iteration as described in the previous section (\ref{sec:RLTheory}). The state space is often so large however - and this certainly is the case for pretty much all \textbf{SP} and \textbf{RC}, except maybe the smallest dimensional \textbf{SP}s - that we cannot practically perform the value iteration procedure for all states. This is where \textbf{DL} comes again to the rescue to act as function approximator, combining with \textbf{RL} into what has become known as \textbf{DRL}. The subtlety is that both the left-hand-side and right hand side in the value-iteration procedure are given by a \textbf{DL} network which we are constantly tweaking. For my implementation of \textbf{DRL}, I followed the same approach as in \cite{Mnih2013}, utilising two loops. During iteration over the first (inner) loop, the right hand-side $V$ of the value-function update equation is computed using a fixed \textit{target-network}. The left hand side $V$ is using another \textit{current-network} which alone is modified by the \textbf{DL} learning (backward propagation and optimization), until convergence is deemed reached. When convergence in the inner loop is deemed obtained, we break out of it, and run the outer loop, which updates the \textit{target-network} via a copy of the \textit{current-network}. The outer loop is itself broken out of when deemed appropriate (some kind of convergence criteria). See pseudo-code \ref{alg:TheoryDRL} below:
\\
\\
\teal
\begin{algorithm}[H]
\caption{Deep Reinforcement Learning -- Heuristic}\label{alg:TheoryDRL}
\begin{algorithmic}
\Function{DeepReinforcementLearningHeuristic}{**args}
\State{$puzzle\_type \gets$ \Call{FromParams}{**args}}
\State{$ puzzles\_gen\_params \gets$ \Call{FromParams}{**args}}
\State{$puzzles\_generation\_criteria \gets$ \Call{FromParams}{**args}}
\State{$n\_puzzles \gets \gets$ \Call{FromParams}{**args}}
\State{$target\_network\_update\_params \gets$ \Call{FromParams}{**args}}
\State{$max\_epochs \gets$ \Call{FromParams}{**args}}
\State{$loss\_function \gets$ \Call{FromParams}{**args}}
\State{$net\_architecture \gets$ \Call{FromParams}{**args}}
\\ \teal{\algorithmiccomment{initial network and target network}}\black
\State {$net \gets \Call{GetNetwork}{puzzle\_type, network\_architecture}$}
\State {$target\_net \gets \Call{GetNetwork}{puzzle\_type, network\_architecture}$}
\State{$epoch \gets 0$}
\State{$puzzles \gets null$}

\While{epoch < max\_epochs \&\& other\_convergence\_criteria(network)}
        \State{$epoch \gets epoch + 1$}
\\ \teal{\algorithmiccomment{generate \textit{training} puzzles if criteria met}}\black
        \If{puzzles\_generation\_criteria($puzzles$)}
            \State{$puzzles \gets generate\_random\_states(puzzle\_type)$}
        \EndIf
        \State{$V \gets dict()$}
        \For{$puzzle \in puzzles}$
            \State{$V[puzzle] \gets \infty$}
            \If \Call{IsGoal}{$puzzle$}:
                \State{$V[puzzle] \gets 0$}
            \Else:
	     \For{$transition, child \in \Call{Children}{$puzzle$}$}
\\ \teal{\algorithmiccomment{perform value iteration}}\black
		\State{$child_cost \gets \Call{Cost}{transition} + target\_net(child)$}
                	\State{$V[puzzle] \gets \Call{Min}{V[puzzle], child\_cost}$}
	     \EndFor
	\EndIf
       \EndFor
\\ \teal{\algorithmiccomment{iterate through network training running feed-forward and backward-prop}}\black
	\State{$net \gets \Call{TrainNeuralNet}{V, loss\_function, net\_architecture}$}
\\ \teal{\algorithmiccomment{update target network if criteria met}}\black
	\If{$target\_network\_update\_criteria(net, target\_net)$}
		\State{$target\_net \gets \Call{Copy}{net}$}
	\EndIf
    \Return{$net$}
\EndWhile


\EndFunction
\end{algorithmic}
\end{algorithm}
\black


%----------------------------------------------------------------------------------------
%	SECTION 5
%----------------------------------------------------------------------------------------

\Section{Deep Q-Learning}
\label{sec:TheoryDQL}
Another fundamental concept of \textbf{RL} is that of the Q-function $Q(s, a)$, the maximum expected reward the agent can obtain from state $s$, taking action $a$ and acting optimally afterwards. Notice that the knowledge of $V$ everywhere is equivalent to that of $Q$, since obviously:
\begin{equation} \label{eq:QI}
V(s) = max_{a \in \mathcal{A}(s)} Q(s, a)
\end{equation}
where $\mathcal{A}(s)$ are the actions from state $s$. Conversely, we can recover $Q$ from $V$ since:
\begin{equation} \label{eq:QI2}
Q(s, a) = R(a) + V(a(s))
\end{equation}
where $R(a)$ is the reward of performing action $a$ and $a(s)$ is the state we reach by performing action $a$ from state $s$. Similarly to the way one can compute $V$ via value-iteration (at least in finite state and action spaces), $Q$ can also be computed by iteration (See \cite{Watkins1992} for details of convergence of that procedure).
\\
\\
Similarly to our discussion about \textbf{DRL}, in cases where the state space is large, it can be useful to approximate $Q$, or a somewhat related quantity (such as e.g. a probability distribution over the actions that can be taken from $s$) by a neural network, and use a similar procedure to that of the previous section to update the right-hand-side and left-hand-side of the iterations. That becomes \textbf{DQL}!
\\
\\
I have for this project followed the exact same procedure described in \cite{https://doi.org/10.48550/arxiv.1805.07470} (and also used by alpha-go \cite{AlphaGo}): the output of the neural network that is learnt by \textbf{DQL}, which actually is surprisingly similar to the approach and pseudo code shown for \textbf{DRL} earlier. The only 3 differences are that:
\begin{itemize}
\item the networks (target and current) now output a vector of size $a$ + 1, where $a$ is the number of possible actions/moves. That is, one dimension for each of the possible actions, representing the probability/desirability of that action, and one dimension for the value-function's value.
\item the target update by Q-iteration now needs to update both the value-function and the optimal decision ($a$-dimensional vector of 0s, with a 1 for the optimal decision only)
\item the loss function used need to be compatible with the size of the output and target vectors. I have used as loss the average of the mean square error of the value function difference (current network and target) and the cross entropy loss of the actions probability versus the actual optimal move.
\end{itemize}
Since in my \textbf{DRL} pseudo-code above (as well as in my actual code), the loss function is abstracted, and the network architecture (including the size of the output) configurable, the only modification to get \textbf{DQL} in lieu of \textbf{DRL} are extremely minimal, and essentially only to do with the Q-iteration-update section:



\teal
\begin{algorithm}[H]
\caption{Deep Q Learning -- Heuristic}\label{alg:TheoryDQL}
\begin{algorithmic}

\Function{DeepReinforcementLearningHeuristic}{**args}

\\ \teal{\algorithmiccomment{ ... same params and init as for \textbf{DRL} ...}}\black
 \State{$nb\_actions \gets$ \Call{GetNbActions}{puzzle\_type}}
\While{epoch ...}
\\ \teal{\algorithmiccomment{ ... same while epoch loop as for \textbf{DRL} ...}}\black
\\ \teal{\algorithmiccomment{ ... update cost \textbf{and actions} via V \& Q iteration ...}}\black
	\State{$Q\_and\_V \gets dict()$}
	\For {$puzzle \in puzzles$}
            \State{$value \gets target\_network(puzzle).value$}
            \State{$actions = [0] * nb\_actions$}
            \State{$best\_action\_id \gets 0$}
	 \If{\Call{IsGoal}{$puzzle$}}
                \State{$value \gets 0$}
            \Else:
                \For{$action\_id, child \in \Call{Children}{puzzle}$}
                    \State{$child\_value \gets target\_network(child).value$}
                    \If{$child\_value \le value$}:
                        \State{$value \gets child\_value$}
                        \State{$best\_action\_id \gets action\_id$}
		\EndIf
		\EndFor
	\EndIf
\\ \teal{\algorithmiccomment{ ... update cost \textbf{and best action} ...}}\black
            \State{$actions[best\_action\_id] \gets 1$}
            \State{$Q\_and\_V[puzzle] \gets actions + [value]$}
	\EndFor
\EndWhile
\EndFunction
\end{algorithmic}
\end{algorithm}
\black

%----------------------------------------------------------------------------------------
%	SECTION 6
%----------------------------------------------------------------------------------------

\Section{Monte Carlo Tree Search}
\label{sec:TheoryMCTS}

The \textbf{DQL} heuristics from \ref{alg:TheoryDQL} no longer only produce cost-to-go estimates as \textbf{DRL}, but rather a joint cost-to-go and distribution over actions (i.e. a 5-dimensional vector for the \textbf{SP} and a 13-dimensional vector for the \textbf{RC}). This leaves us with the question of how can we use the output of such a network to search for a solution. One simple answer (which I have tried, see e.g. subsection \ref{sec:S33DRLDQL}) is that we could disregard the distribution and only take into account the cost-to-go by simply using $A^{*}$. The rationale for doing so is that we might hope that jointly learning the cost-to-go and the actions via a combination of MSE and cross-entropy-loss produces a better network and leads to better heuristics, since we use more information during the learning and iterations.
\\
\\
Another possibility is to use an algorithm combining the actions distribution and the value function to inform its search. The Monte Carlo Tree Search (\textbf{MCTS}) is such an algorithm. It generates at each iteration (possibly in a distributed or multithreaded manner) paths to a \textit{leaf} on the frontier of unexpanded nodes, expands that node and checks if it is a goal state (and keeps going until the goal is found). The paths followed from initial state to leaves are decided via a combination of the probability distribution over actions (allowing exploration) and value function (exploitation), according to a trade-off which can be tuned via a hyper-parameter $c$. To prevent following the same paths (which might lead the search far from the goal), the \textbf{MCTS} keeps track of actions already followed and downweight their probability so that subsequent paths explore different actions. Let me first present in pseudo-code the paths generation, in particular the trade-off between actions-values (exploration-exploitation):
\\


\teal
\begin{algorithm}[H]
\caption{Monte Carlo Tree Search -- Child Node Choice}\label{alg:TheoryDQLNodeChoice}
\begin{algorithmic}

\Function{ChooseChildNode}{$node, actions\_probas, actions\_values, actions\_count, c$}

\\ \teal{\algorithmiccomment{c is the actions-values trade-off hyper-parameter}}\black

\State{$N \gets \sqrt{\sum_{n \in actions\_count}{n}}$}

\\ \teal{\algorithmiccomment{... frequent actions penalized as denominator grows faster than numerator N ... }}\black
\State{$Q \gets N . \frac{actions\_probas}{actions\_count}$}
\State{$V \gets actions\_values$}
\\ \teal{\algorithmiccomment{... high c gives weight to actions to favour exploration ... }}\black
\\ \teal{\algorithmiccomment{... return child for which cQ + V is maximized ... }}\black
\State{$action\_id \gets \Call{IndexMax}{c.Q + V}$}
\\
\Return{\Call{Child}{$node, action\_id$}}
\EndFunction
\end{algorithmic}
\end{algorithm}
\black

\noindent The actual \textbf{MCTS} algorithm is simply generating a new path from root to a new leaf at each iteration, until the goal is found:

\teal
\begin{algorithm}[H]
\caption{Monte Carlo Tree Search}\label{alg:TheoryMCTS}
\begin{algorithmic}
\Function{MCTS}{$root, time\_out, q\_v\_network, c$}
\State{\Call{InitialiseTimeOut}{$time\_out$}}
\State{$tree \gets \Call{Tree}{root}$}
\If{\Call{IsGoal}{$root$}}
  \Return{$root$}
\EndIf
\While{true}
	\State\Call{CheckTimeOut}{void}
	\State{$leaf \gets root$}
	\While{$leaf.expanded$}
        		\State{$actions\_probas, actions\_values \gets q\_v\_network(leaf)$}
	           \State{$leaf \gets\Call{ChooseChildNode}{leaf,  actions\_probas, actions\_values, leaf.actions\_count, c}$}
		 \State{\Call{Expand}{$leaf, tree$}}
	\EndWhile
           \Call{IncrementActionsFromLeafToRoot}{$leaf$}
       \\ \teal{\algorithmiccomment{path to leaf can usually be improved by re-running \textbf{BFS} on the resulting tree}}\black
       \If{\Call{IsGoal}{$leaf$}}
            \Return{\Call{BlindSearch}{$tree, search\_type=Search.BFS$}}
       \EndIf
\EndWhile
\EndFunction
\end{algorithmic}
\end{algorithm}
\black

\noindent Note that for this project, I have implemented all the above algorithms in Python (BFS, DFS, A$^{*}$, DeepLearner, DeepReinforcementLearner, DeepQLearner, MonteCarloTreeSearch), and more details can be found in section (\ref{sec:Implementation}).




































